{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cd50fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.7/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      "com.amazonaws#aws-java-sdk-bundle added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-b17a9880-e1a2-421c-98df-ac59f15da9ed;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.4 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.12.262 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      ":: resolution report :: resolve 512ms :: artifacts dl 11ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.12.262 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.4 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\t:: evicted modules:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.901 by [com.amazonaws#aws-java-sdk-bundle;1.12.262] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   4   |   0   |   0   |   1   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-b17a9880-e1a2-421c-98df-ac59f15da9ed\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/7ms)\n",
      "25/11/01 23:49:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "/usr/local/lib/python3.7/site-packages/pyspark/context.py:317: FutureWarning: Python 3.7 support is deprecated in Spark 3.4.\n",
      "  warnings.warn(\"Python 3.7 support is deprecated in Spark 3.4.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verificando e excluindo o arquivo final anterior em: s3a://bucket-client-upa-connect-sofh/basesExternas/BasesExternasIntegradas.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/01 23:49:52 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo anterior BasesExternasIntegradas.csv excluído com sucesso.\n",
      "Iniciando a leitura e padronização dos DataFrames do bucket trusted...\n",
      "Lendo atendimentos de: s3a://bucket-trusted-upa-connect-sofh/BasesExternas/Atendimentos/AtendimentosTratada.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lendo clima_tempo de: s3a://bucket-trusted-upa-connect-sofh/BasesExternas/ClimaTempo/ClimaTempoTratada.csv\n",
      "Lendo febre_amarela de: s3a://bucket-trusted-upa-connect-sofh/BasesExternas/FebreAmarela/FebreAmarelaTratada.csv\n",
      "Lendo sindrome_gripal de: s3a://bucket-trusted-upa-connect-sofh/BasesExternas/SindromeGripal/sindromeGripalTratada.csv\n",
      "Leitura e preparação concluídas.\n",
      "Chave de junção padronizada (data e hora): DATA_HORA\n",
      "\n",
      "Iniciando a junção dos DataFrames...\n",
      "Junção com clima_tempo concluída.\n",
      "Junção com febre_amarela concluída.\n",
      "Junção com sindrome_gripal concluída.\n",
      "Junção de todos os DataFrames concluída.\n",
      "\n",
      "Padronizando nomes das colunas para snake_case...\n",
      "root\n",
      " |-- data_hora: string (nullable = true)\n",
      " |-- atendimentos_sexo: string (nullable = true)\n",
      " |-- atendimentos_tipo_de_unidade: string (nullable = true)\n",
      " |-- atendimentos_descricao_do_procedimento: string (nullable = true)\n",
      " |-- atendimentos_descricao_do_cbo: string (nullable = true)\n",
      " |-- atendimentos_nacionalidade: string (nullable = true)\n",
      " |-- atendimentos_tipo_procedimento: string (nullable = true)\n",
      " |-- clima_tempo_temperature_2_m: string (nullable = true)\n",
      " |-- clima_tempo_relative_humidity_2_m: string (nullable = true)\n",
      " |-- clima_tempo_rain: string (nullable = true)\n",
      " |-- clima_tempo_precipitation: string (nullable = true)\n",
      " |-- clima_tempo_pm10: string (nullable = true)\n",
      " |-- clima_tempo_pm2_5: string (nullable = true)\n",
      " |-- clima_tempo_carbon_monoxide: string (nullable = true)\n",
      " |-- clima_tempo_nitrogen_dioxide: string (nullable = true)\n",
      " |-- clima_tempo_ozone: string (nullable = true)\n",
      " |-- clima_tempo_sulphur_dioxide: string (nullable = true)\n",
      " |-- febre_amarela_uf_lpi: string (nullable = true)\n",
      " |-- febre_amarela_mun_lpi: string (nullable = true)\n",
      " |-- febre_amarela_sexo: string (nullable = true)\n",
      " |-- febre_amarela_idade: string (nullable = true)\n",
      " |-- febre_amarela_ano_is: string (nullable = true)\n",
      " |-- febre_amarela_obito: string (nullable = true)\n",
      " |-- febre_amarela_dt_obito: string (nullable = true)\n",
      " |-- sindrome_gripal_id: string (nullable = true)\n",
      " |-- sindrome_gripal_idade: string (nullable = true)\n",
      " |-- sindrome_gripal_sexo: string (nullable = true)\n",
      " |-- sindrome_gripal_sintoma_individual: string (nullable = true)\n",
      " |-- sindrome_gripal_bairro: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de linhas na base integrada: 1307095\n",
      "\n",
      "Escrevendo dados temporariamente em: s3a://bucket-client-upa-connect-sofh/basesExternas/_temp_staging_integrated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/01 23:50:28 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "25/11/01 23:50:40 WARN AbstractS3ACommitterFactory: Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.\n",
      "25/11/01 23:50:41 WARN AbstractS3ACommitterFactory: Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Base integrada salva e renomeada com sucesso para: s3a://bucket-client-upa-connect-sofh/basesExternas/BasesExternasIntegradas.csv\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date, date_format, lit, concat\n",
    "import re # Adicionado a importação de re\n",
    "\n",
    "# --- INÍCIO: FUNÇÃO AUXILIAR SNAKE_CASE ---\n",
    "def to_snake_case(name):\n",
    "    \"\"\"\n",
    "    Converte uma string (CamelCase, UPPER_CASE, etc.) para snake_case.\n",
    "    Ex: \"DataRegistro\" -> \"data_registro\"\n",
    "    Ex: \"ATENDIMENTOS_SEXO\" -> \"atendimentos_sexo\"\n",
    "    Ex: \"CLIMA_TEMPO_PM2_5\" -> \"clima_tempo_pm2_5\"\n",
    "    \"\"\"\n",
    "    # Insere _ antes de letras maiúsculas (ex: DataRegistro -> Data_Registro)\n",
    "    s1 = re.sub('(.)([A-Z][a-z]+)', r'\\1_\\2', name)\n",
    "    # Insere _ antes de grupos de letras maiúsculas/números (ex: CBO, PM2_5)\n",
    "    s2 = re.sub('([a-z0-9])([A-Z])', r'\\1_\\2', s1)\n",
    "    # Converte tudo para minúsculo\n",
    "    return s2.lower()\n",
    "# --- FIM: FUNÇÃO AUXILIAR SNAKE_CASE ---\n",
    "\n",
    "# =======================================================================\n",
    "# 1. CONFIGURAÇÃO E INICIALIZAÇÃO DO SPARK\n",
    "# =======================================================================\n",
    "conf = SparkConf()\n",
    "conf.set(\n",
    "    \"spark.jars.packages\",\n",
    "    \"org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.11.901\"\n",
    ")\n",
    "conf.set(\n",
    "    \"spark.hadoop.fs.s3a.aws.credentials.provider\",\n",
    "    \"com.amazonaws.auth.InstanceProfileCredentialsProvider\"\n",
    ")\n",
    "\n",
    "spark = SparkSession.builder.appName(\"ExternalDataIntegration\").config(conf=conf).getOrCreate()\n",
    "\n",
    "# =======================================================================\n",
    "# 2. DEFINIÇÃO DOS CAMINHOS DOS ARQUIVOS DE ENTRADA E SAÍDA\n",
    "# =======================================================================\n",
    "\n",
    "# Arquivos de Entrada no bucket trusted\n",
    "# (Mantidos os nomes dos arquivos conforme o script original)\n",
    "INPUT_PATHS = {\n",
    "    \"atendimentos\": \"s3a://bucket-trusted-upa-connect-sofh/BasesExternas/Atendimentos/AtendimentosTratada.csv\",\n",
    "    \"clima_tempo\": \"s3a://bucket-trusted-upa-connect-sofh/BasesExternas/ClimaTempo/ClimaTempoTratada.csv\",\n",
    "    \"febre_amarela\": \"s3a://bucket-trusted-upa-connect-sofh/BasesExternas/FebreAmarela/FebreAmarelaTratada.csv\",\n",
    "    \"sindrome_gripal\": \"s3a://bucket-trusted-upa-connect-sofh/BasesExternas/SindromeGripal/sindromeGripalTratada.csv\",\n",
    "}\n",
    "\n",
    "# Caminho de Saída no bucket client\n",
    "FINAL_OUTPUT_DIR = \"s3a://bucket-client-upa-connect-sofh/basesExternas\"\n",
    "FINAL_FILENAME = \"BasesExternasIntegradas.csv\"\n",
    "TEMP_STAGING_DIR = f\"{FINAL_OUTPUT_DIR}/_temp_staging_integrated\"\n",
    "FINAL_OUTPUT_PATH = f\"{FINAL_OUTPUT_DIR}/{FINAL_FILENAME}\" # Adicionado: Caminho completo do arquivo final\n",
    "\n",
    "# Coluna de Junção Comum (AGORA COM HORA INCLUÍDA)\n",
    "JOIN_COLUMN = \"DATA_HORA\"\n",
    "\n",
    "# =======================================================================\n",
    "# 2.1. EXCLUSÃO DO ARQUIVO FINAL ANTERIOR\n",
    "# =======================================================================\n",
    "\n",
    "print(f\"Verificando e excluindo o arquivo final anterior em: {FINAL_OUTPUT_PATH}\")\n",
    "try:\n",
    "    # Acessa as classes Java do Hadoop para manipulação de arquivos no S3\n",
    "    Path = spark._jvm.org.apache.hadoop.fs.Path\n",
    "    hadoop_conf = spark._jsc.hadoopConfiguration()\n",
    "    fs = Path(FINAL_OUTPUT_PATH).getFileSystem(hadoop_conf)\n",
    "\n",
    "    if fs.exists(Path(FINAL_OUTPUT_PATH)):\n",
    "        # Tenta excluir o arquivo (não recursivo, pois é esperado que seja um arquivo)\n",
    "        fs.delete(Path(FINAL_OUTPUT_PATH), False)\n",
    "        print(f\"Arquivo anterior {FINAL_FILENAME} excluído com sucesso.\")\n",
    "    else:\n",
    "        print(f\"Arquivo anterior {FINAL_FILENAME} não encontrado. Prosseguindo.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nATENÇÃO: Não foi possível verificar/excluir o arquivo final no S3. O script pode falhar na etapa de salvar/renomear: {e}\")\n",
    "\n",
    "# =======================================================================\n",
    "# 3. LEITURA DOS DADOS E PREPARAÇÃO PARA JOIN (COM DATA E HORA)\n",
    "# =======================================================================\n",
    "\n",
    "dataframes = {}\n",
    "\n",
    "print(\"Iniciando a leitura e padronização dos DataFrames do bucket trusted...\")\n",
    "for name, path in INPUT_PATHS.items():\n",
    "    print(f\"Lendo {name} de: {path}\")\n",
    "    \n",
    "    # Define as opções de leitura padrão\n",
    "    read_options = {\n",
    "        \"header\": \"true\",\n",
    "        \"encoding\": \"UTF-8\"\n",
    "    }\n",
    "\n",
    "    # Define delimitador específico.\n",
    "    if name in [\"atendimentos\", \"clima_tempo\", \"febre_amarela\", \"sindrome_gripal\"]:\n",
    "        read_options[\"delimiter\"] = \";\"\n",
    "        \n",
    "    # Leitura de CSV\n",
    "    df = spark.read.options(**read_options).csv(path)\n",
    "    \n",
    "    # --- NOVO BLOCO: Padronização da Coluna de Data/Hora (JOIN_COLUMN) ---\n",
    "    \n",
    "    # As bases \"atendimentos\" e \"clima_tempo\" possuem a coluna DATA_ISO (Data e Hora)\n",
    "    if name in [\"atendimentos\", \"clima_tempo\"]:\n",
    "        # Usa DATA_ISO (ISO 8601) diretamente no formato YYYY-MM-DD HH:MM:SS\n",
    "        df = df.withColumn(\n",
    "            JOIN_COLUMN,\n",
    "            date_format(col(\"DATA_ISO\"), \"yyyy-MM-dd HH:mm:ss\") # Mantém a hora\n",
    "        ).drop(\"DATA_ISO\")\n",
    "        \n",
    "    # As bases \"febre_amarela\" e \"sindrome_gripal\" são diárias (SÓ DATA)\n",
    "    elif name == \"febre_amarela\":\n",
    "        # 1. Converte a coluna DT_IS (DD/MM/YYYY) para a data padronizada (YYYY-MM-DD)\n",
    "        df = df.withColumn(\n",
    "            \"DATA_PADRAO\",\n",
    "            date_format(to_date(col(\"DT_IS\"), \"dd/MM/yyyy\"), \"yyyy-MM-dd\")\n",
    "        ).drop(\"DT_IS\")\n",
    "        \n",
    "        # 2. Concatena com a hora '00:00:00'\n",
    "        df = df.withColumn(\n",
    "            JOIN_COLUMN,\n",
    "            concat(col(\"DATA_PADRAO\"), lit(\" 00:00:00\")) # Adiciona a hora 00:00:00\n",
    "        ).drop(\"DATA_PADRAO\")\n",
    "        \n",
    "    elif name == \"sindrome_gripal\":\n",
    "        # DATA_NOTIFICACAO_TRATADA já está em YYYY-MM-DD (tratada anteriormente)\n",
    "        # 1. Renomeia temporariamente para DATA_PADRAO\n",
    "        df = df.withColumnRenamed(\"DATA_NOTIFICACAO_TRATADA\", \"DATA_PADRAO\")\n",
    "        \n",
    "        # 2. Concatena com a hora '00:00:00'\n",
    "        df = df.withColumn(\n",
    "            JOIN_COLUMN,\n",
    "            concat(col(\"DATA_PADRAO\"), lit(\" 00:00:00\")) # Adiciona a hora 00:00:00\n",
    "        ).drop(\"DATA_PADRAO\")\n",
    "        \n",
    "        \n",
    "    # --- FIM NOVO BLOCO ---\n",
    "        \n",
    "    # Renomeia colunas para evitar conflitos (exceto a coluna de junção)\n",
    "    for c in df.columns:\n",
    "        if c.upper() != JOIN_COLUMN.upper():\n",
    "            # Aplica o prefixo em caixa alta (ex: ATENDIMENTOS_ID_PACIENTE)\n",
    "            # Nota: O nome 'id' da sindrome gripal vira 'SINDROME_GRIPAL_id'\n",
    "            # Isso será corrigido pela função snake_case no final\n",
    "            df = df.withColumnRenamed(c, f\"{name.upper()}_{c}\")\n",
    "            \n",
    "    # Seleciona o DataFrame com a nova coluna de junção\n",
    "    dataframes[name] = df\n",
    "\n",
    "print(\"Leitura e preparação concluídas.\")\n",
    "print(f\"Chave de junção padronizada (data e hora): {JOIN_COLUMN}\")\n",
    "\n",
    "# =======================================================================\n",
    "# 4. REALIZAÇÃO DA JUNÇÃO (FULL OUTER JOIN)\n",
    "# =======================================================================\n",
    "print(\"\\nIniciando a junção dos DataFrames...\")\n",
    "\n",
    "# Começa com o DataFrame de Atendimentos\n",
    "df_base = dataframes[\"atendimentos\"]\n",
    "\n",
    "# Realiza a junção com os demais DataFrames\n",
    "for name, df_join in dataframes.items():\n",
    "    if name != \"atendimentos\":\n",
    "        # Full outer join para manter a maior granularidade possível (a data e hora)\n",
    "        df_base = df_base.join(df_join, on=JOIN_COLUMN, how=\"full_outer\")\n",
    "        print(f\"Junção com {name} concluída.\")\n",
    "\n",
    "print(\"Junção de todos os DataFrames concluída.\")\n",
    "\n",
    "# =======================================================================\n",
    "# 5. REMOÇÃO DE COLUNAS DE ID E PADRONIZAÇÃO FINAL PARA SNAKE_CASE\n",
    "# =======================================================================\n",
    "\n",
    "# 1. REMOÇÃO DE COLUNAS DE ID\n",
    "# Colunas a serem removidas do resultado final\n",
    "# **** CORREÇÃO APLICADA AQUI ****\n",
    "# Mantido SINDROME_GRIPAL_id (que virou SINDROME_GRIPAL_ID ou SINDROME_GRIPAL_id)\n",
    "# Removido apenas FEBRE_AMARELA_ID\n",
    "COLUNAS_PARA_REMOVER = [\n",
    "    \"FEBRE_AMARELA_ID\"\n",
    "]\n",
    "\n",
    "# Cria uma lista de colunas para manter, excluindo as colunas de ID\n",
    "colunas_para_renomear = [c for c in df_base.columns if c.upper() not in COLUNAS_PARA_REMOVER]\n",
    "\n",
    "# Aplica a seleção\n",
    "df_base = df_base.select(colunas_para_renomear)\n",
    "\n",
    "# 2. PADRONIZAÇÃO PARA SNAKE_CASE\n",
    "print(\"\\nPadronizando nomes das colunas para snake_case...\")\n",
    "\n",
    "# Cria um mapa de colunas antigas para novas (snake_case)\n",
    "col_rename_map = {c: to_snake_case(c) for c in df_base.columns}\n",
    "\n",
    "# Aplica a renomeação a todas as colunas\n",
    "for old_name, new_name in col_rename_map.items():\n",
    "    df_base = df_base.withColumnRenamed(old_name, new_name)\n",
    "\n",
    "# Renomeia a coluna de junção para 'data_hora' (que será 'data_hora' em snake_case)\n",
    "# Se o JOIN_COLUMN já for \"DATA_HORA\", ele vira \"data_hora\" aqui.\n",
    "df_base.printSchema()\n",
    "print(f\"Total de linhas na base integrada: {df_base.count()}\")\n",
    "\n",
    "# =======================================================================\n",
    "# 6. SALVANDO E RENOMEANDO O RESULTADO NO S3 (BUCKET CLIENT)\n",
    "# =======================================================================\n",
    "\n",
    "# 1. Escreve o resultado no caminho temporário\n",
    "print(f\"\\nEscrevendo dados temporariamente em: {TEMP_STAGING_DIR}\")\n",
    "\n",
    "# NOTA: Coalesce(1) para garantir a geração de um único arquivo CSV.\n",
    "df_base.coalesce(1).write \\\n",
    "    .option('delimiter', ';') \\\n",
    "    .option('header', 'true') \\\n",
    "    .option('encoding', 'UTF-8') \\\n",
    "    .mode('overwrite') \\\n",
    "    .csv(TEMP_STAGING_DIR)\n",
    "\n",
    "# 2. Renomeia o arquivo gerado\n",
    "try:\n",
    "    # Acessa a classe 'Path' da JVM através do gateway do Spark\n",
    "    Path = spark._jvm.org.apache.hadoop.fs.Path\n",
    "    \n",
    "    # Acessa a configuração do Hadoop\n",
    "    hadoop_conf = spark._jsc.hadoopConfiguration()\n",
    "    \n",
    "    # Obtém o objeto FileSystem para o caminho temporário\n",
    "    fs = Path(TEMP_STAGING_DIR).getFileSystem(hadoop_conf)\n",
    "\n",
    "    # Encontra o arquivo gerado (part-00000-*.csv) dentro do diretório temporário\n",
    "    list_status = fs.globStatus(Path(TEMP_STAGING_DIR + \"/part-00000-*.csv\"))\n",
    "\n",
    "    if list_status:\n",
    "        # Pega o caminho completo do arquivo gerado\n",
    "        generated_file_path = list_status[0].getPath()\n",
    "\n",
    "        # Define o caminho final e o nome específico para o arquivo\n",
    "        final_output_path = Path(f\"{FINAL_OUTPUT_DIR}/{FINAL_FILENAME}\")\n",
    "\n",
    "        # Renomeia (move) o arquivo para o caminho e nome definitivos\n",
    "        fs.rename(generated_file_path, final_output_path)\n",
    "        \n",
    "        # 3. Deleta o diretório temporário (que ficou vazio) e outros arquivos de metadados\n",
    "        fs.delete(Path(TEMP_STAGING_DIR), True)\n",
    "        \n",
    "        print(f\"\\n✅ Base integrada salva e renomeada com sucesso para: {final_output_path}\")\n",
    "\n",
    "    else:\n",
    "        print(\"\\nErro: Não foi possível encontrar o arquivo CSV gerado (part-00000-*.csv) no caminho temporário.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nOcorreu um erro durante a renomeação do arquivo no S3: {e}\")\n",
    "\n",
    "# Encerra a sessão Spark\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c428cc0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
