{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56b808ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.7/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      "com.amazonaws#aws-java-sdk-bundle added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-3ed9a50e-cdf0-4bb3-af29-e8805f05fdb9;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.4 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.12.262 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      ":: resolution report :: resolve 479ms :: artifacts dl 19ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.12.262 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.4 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\t:: evicted modules:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.901 by [com.amazonaws#aws-java-sdk-bundle;1.12.262] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   4   |   0   |   0   |   1   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-3ed9a50e-cdf0-4bb3-af29-e8805f05fdb9\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/11ms)\n",
      "25/11/02 00:20:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "/usr/local/lib/python3.7/site-packages/pyspark/context.py:317: FutureWarning: Python 3.7 support is deprecated in Spark 3.4.\n",
      "  warnings.warn(\"Python 3.7 support is deprecated in Spark 3.4.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verificando e excluindo o arquivo final anterior em: s3a://bucket-client-upa-connect-sofh/avaliacoes/GoogleTwitterIntegrados.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/02 00:21:02 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo anterior GoogleTwitterIntegrados.csv não encontrado. Prosseguindo.\n",
      "Iniciando a leitura e padronização dos DataFrames (Google e Twitter)...\n",
      "Lendo google de: s3a://bucket-trusted-upa-connect-sofh/BasesExternas/Google/part-00000-61facf5d-5dac-44c5-bda2-eb82fc4c171b-c000.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema de google após preparação:\n",
      "root\n",
      " |-- GOOGLE_Nome_Upa: string (nullable = true)\n",
      " |-- GOOGLE_Contagem_Avaliacoes: string (nullable = true)\n",
      " |-- GOOGLE_Pontuacao_Total: string (nullable = true)\n",
      " |-- GOOGLE_Cidade: string (nullable = true)\n",
      " |-- GOOGLE_Estado: string (nullable = true)\n",
      " |-- GOOGLE_Bairro: string (nullable = true)\n",
      " |-- GOOGLE_Rua: string (nullable = true)\n",
      " |-- GOOGLE_CEP: string (nullable = true)\n",
      " |-- GOOGLE_Origem_Avaliacao: string (nullable = true)\n",
      " |-- GOOGLE_Data_Coleta: string (nullable = true)\n",
      " |-- DataRegistro: string (nullable = true)\n",
      " |-- GOOGLE_Texto_Avaliacao: string (nullable = true)\n",
      " |-- GOOGLE_Estrelas: string (nullable = true)\n",
      "\n",
      "Lendo twitter de: s3a://bucket-trusted-upa-connect-sofh/BasesExternas/Twitter/sentimentos_encontrados.csv\n",
      "Schema de twitter após preparação:\n",
      "root\n",
      " |-- TWITTER_palavra: string (nullable = true)\n",
      " |-- TWITTER_tipo: string (nullable = true)\n",
      " |-- DataRegistro: string (nullable = true)\n",
      "\n",
      "Leitura e preparação concluídas.\n",
      "\n",
      "Iniciando a junção dos DataFrames...\n",
      "Junção entre Google e Twitter concluída.\n",
      "\n",
      "Padronizando nomes das colunas para snake_case...\n",
      "root\n",
      " |-- data_registro: string (nullable = true)\n",
      " |-- google__nome__upa: string (nullable = true)\n",
      " |-- google__contagem__avaliacoes: string (nullable = true)\n",
      " |-- google__pontuacao__total: string (nullable = true)\n",
      " |-- google__cidade: string (nullable = true)\n",
      " |-- google__estado: string (nullable = true)\n",
      " |-- google__bairro: string (nullable = true)\n",
      " |-- google__rua: string (nullable = true)\n",
      " |-- google_cep: string (nullable = true)\n",
      " |-- google__origem__avaliacao: string (nullable = true)\n",
      " |-- google__data__coleta: string (nullable = true)\n",
      " |-- google__texto__avaliacao: string (nullable = true)\n",
      " |-- google__estrelas: string (nullable = true)\n",
      " |-- twitter_palavra: string (nullable = true)\n",
      " |-- twitter_tipo: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de linhas na base integrada (Google/Twitter): 3702\n",
      "\n",
      "Escrevendo dados temporariamente em: s3a://bucket-client-upa-connect-sofh/avaliacoes/_temp_staging_gt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/02 00:21:21 WARN AbstractS3ACommitterFactory: Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.\n",
      "25/11/02 00:21:22 WARN AbstractS3ACommitterFactory: Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Base integrada salva e renomeada com sucesso para: s3a://bucket-client-upa-connect-sofh/avaliacoes/GoogleTwitterIntegrados.csv\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date, date_format\n",
    "import re \n",
    "\n",
    "# --- INÍCIO: FUNÇÃO AUXILIAR SNAKE_CASE ---\n",
    "def to_snake_case(name):\n",
    "    \"\"\"\n",
    "    Converte uma string (CamelCase, UPPER_CASE, etc.) para snake_case.\n",
    "    \"\"\"\n",
    "    # Insere _ antes de letras maiúsculas\n",
    "    s1 = re.sub('(.)([A-Z][a-z]+)', r'\\1_\\2', name)\n",
    "    # Insere _ antes de grupos de letras maiúsculas/números\n",
    "    s2 = re.sub('([a-z0-9])([A-Z])', r'\\1_\\2', s1)\n",
    "    # Converte tudo para minúsculo\n",
    "    return s2.lower()\n",
    "# --- FIM: FUNÇÃO AUXILIAR SNAKE_CASE ---\n",
    "\n",
    "# =======================================================================\n",
    "# 1. CONFIGURAÇÃO E INICIALIZAÇÃO DO SPARK\n",
    "# =======================================================================\n",
    "conf = SparkConf()\n",
    "conf.set(\n",
    "    \"spark.jars.packages\",\n",
    "    \"org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.11.901\"\n",
    ")\n",
    "conf.set(\n",
    "    \"spark.hadoop.fs.s3a.aws.credentials.provider\",\n",
    "    \"com.amazonaws.auth.InstanceProfileCredentialsProvider\"\n",
    ")\n",
    "\n",
    "spark = SparkSession.builder.appName(\"GoogleTwitterIntegration\").config(conf=conf).getOrCreate()\n",
    "\n",
    "# =======================================================================\n",
    "# 2. DEFINIÇÃO DOS CAMINHOS DOS ARQUIVOS DE ENTRADA E SAÍDA\n",
    "# =======================================================================\n",
    "\n",
    "# Arquivos de Entrada no bucket trusted (SOMENTE GOOGLE E TWITTER)\n",
    "INPUT_PATHS = {\n",
    "    \"google\": \"s3a://bucket-trusted-upa-connect-sofh/BasesExternas/Google/part-00000-61facf5d-5dac-44c5-bda2-eb82fc4c171b-c000.csv\",\n",
    "    \"twitter\": \"s3a://bucket-trusted-upa-connect-sofh/BasesExternas/Twitter/sentimentos_encontrados.csv\"\n",
    "}\n",
    "\n",
    "# Caminho de Saída no bucket client\n",
    "FINAL_OUTPUT_DIR = \"s3a://bucket-client-upa-connect-sofh/avaliacoes\"\n",
    "FINAL_FILENAME = \"GoogleTwitterIntegrados.csv\"\n",
    "TEMP_STAGING_DIR = f\"{FINAL_OUTPUT_DIR}/_temp_staging_gt\"\n",
    "FINAL_OUTPUT_PATH = f\"{FINAL_OUTPUT_DIR}/{FINAL_FILENAME}\"\n",
    "\n",
    "# Coluna de Junção Comum (Padronizada para a Data apenas - YYYY-MM-DD)\n",
    "JOIN_COLUMN = \"DataRegistro\"\n",
    "\n",
    "# =======================================================================\n",
    "# 2.1. EXCLUSÃO DO ARQUIVO FINAL ANTERIOR\n",
    "# =======================================================================\n",
    "\n",
    "print(f\"Verificando e excluindo o arquivo final anterior em: {FINAL_OUTPUT_PATH}\")\n",
    "try:\n",
    "    Path = spark._jvm.org.apache.hadoop.fs.Path\n",
    "    hadoop_conf = spark._jsc.hadoopConfiguration()\n",
    "    fs = Path(FINAL_OUTPUT_PATH).getFileSystem(hadoop_conf)\n",
    "\n",
    "    if fs.exists(Path(FINAL_OUTPUT_PATH)):\n",
    "        fs.delete(Path(FINAL_OUTPUT_PATH), False) \n",
    "        print(f\"Arquivo anterior {FINAL_FILENAME} excluído com sucesso.\")\n",
    "    else:\n",
    "        print(f\"Arquivo anterior {FINAL_FILENAME} não encontrado. Prosseguindo.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nATENÇÃO: Não foi possível verificar/excluir o arquivo final no S3. {e}\")\n",
    "\n",
    "# =======================================================================\n",
    "# 3. LEITURA DOS DADOS E PREPARAÇÃO PARA JOIN (Delimitadores Corrigidos)\n",
    "# =======================================================================\n",
    "\n",
    "dataframes = {}\n",
    "\n",
    "print(\"Iniciando a leitura e padronização dos DataFrames (Google e Twitter)...\")\n",
    "for name, path in INPUT_PATHS.items():\n",
    "    print(f\"Lendo {name} de: {path}\")\n",
    "    \n",
    "    read_options = {\n",
    "        \"header\": \"true\",\n",
    "        \"encoding\": \"UTF-8\"\n",
    "    }\n",
    "\n",
    "    # CORREÇÃO DO DELIMITADOR: Google usa ';', Twitter usa ','\n",
    "    if name == \"google\":\n",
    "        read_options[\"delimiter\"] = \";\" \n",
    "    elif name == \"twitter\":\n",
    "        read_options[\"delimiter\"] = \",\"\n",
    "        \n",
    "    df = spark.read.options(**read_options).csv(path)\n",
    "    \n",
    "    # Padronização da Coluna de Data\n",
    "    if name == \"google\":\n",
    "        # Renomeia a coluna real 'Data_Publicacao_Avaliacao' para JOIN_COLUMN\n",
    "        df = df.withColumnRenamed(\"Data_Publicacao_Avaliacao\", JOIN_COLUMN)\n",
    "\n",
    "    elif name == \"twitter\":\n",
    "        # Renomeia a coluna real 'dataPublicacao' para JOIN_COLUMN\n",
    "        df = df.withColumnRenamed(\"dataPublicacao\", JOIN_COLUMN)\n",
    "    \n",
    "    # Renomeia colunas para evitar conflitos (exceto a coluna de junção)\n",
    "    for c in df.columns:\n",
    "        if c.upper() != JOIN_COLUMN.upper():\n",
    "            df = df.withColumnRenamed(c, f\"{name.upper()}_{c}\")\n",
    "            \n",
    "    dataframes[name] = df\n",
    "    print(f\"Schema de {name} após preparação:\")\n",
    "    dataframes[name].printSchema()\n",
    "\n",
    "print(\"Leitura e preparação concluídas.\")\n",
    "\n",
    "# =======================================================================\n",
    "# 4. REALIZAÇÃO DA JUNÇÃO (FULL OUTER JOIN)\n",
    "# =======================================================================\n",
    "print(\"\\nIniciando a junção dos DataFrames...\")\n",
    "\n",
    "df_google = dataframes[\"google\"]\n",
    "df_twitter = dataframes[\"twitter\"]\n",
    "\n",
    "# Junção usando a coluna DataRegistro, que agora existe nos dois DataFrames.\n",
    "df_base = df_google.join(df_twitter, on=JOIN_COLUMN, how=\"full_outer\")\n",
    "print(\"Junção entre Google e Twitter concluída.\")\n",
    "\n",
    "# =======================================================================\n",
    "# 5. REMOÇÃO DE COLUNAS DE ID E PADRONIZAÇÃO FINAL PARA SNAKE_CASE\n",
    "# =======================================================================\n",
    "\n",
    "# 1. REMOÇÃO DE COLUNAS DE ID\n",
    "# Lista vazia, conforme observação de que os IDs não existem\n",
    "COLUNAS_PARA_REMOVER = [] \n",
    "\n",
    "colunas_para_renomear = [c for c in df_base.columns if c not in COLUNAS_PARA_REMOVER]\n",
    "df_base = df_base.select(colunas_para_renomear)\n",
    "\n",
    "# 2. PADRONIZAÇÃO PARA SNAKE_CASE\n",
    "print(\"\\nPadronizando nomes das colunas para snake_case...\")\n",
    "\n",
    "col_rename_map = {c: to_snake_case(c) for c in df_base.columns}\n",
    "\n",
    "for old_name, new_name in col_rename_map.items():\n",
    "    df_base = df_base.withColumnRenamed(old_name, new_name)\n",
    "\n",
    "df_base.printSchema()\n",
    "print(f\"Total de linhas na base integrada (Google/Twitter): {df_base.count()}\")\n",
    "\n",
    "# =======================================================================\n",
    "# 6. SALVANDO E RENOMEANDO O RESULTADO NO S3\n",
    "# =======================================================================\n",
    "\n",
    "# 1. Escreve o resultado no caminho temporário\n",
    "print(f\"\\nEscrevendo dados temporariamente em: {TEMP_STAGING_DIR}\")\n",
    "\n",
    "df_base.coalesce(1).write \\\n",
    "    .option('delimiter', ';') \\\n",
    "    .option('header', 'true') \\\n",
    "    .option('encoding', 'UTF-8') \\\n",
    "    .mode('overwrite') \\\n",
    "    .csv(TEMP_STAGING_DIR)\n",
    "\n",
    "# 2. Renomeia o arquivo gerado\n",
    "try:\n",
    "    Path = spark._jvm.org.apache.hadoop.fs.Path\n",
    "    hadoop_conf = spark._jsc.hadoopConfiguration()\n",
    "    fs = Path(TEMP_STAGING_DIR).getFileSystem(hadoop_conf)\n",
    "\n",
    "    list_status = fs.globStatus(Path(TEMP_STAGING_DIR + \"/part-00000-*.csv\"))\n",
    "\n",
    "    if list_status:\n",
    "        generated_file_path = list_status[0].getPath()\n",
    "        final_output_path = Path(FINAL_OUTPUT_PATH)\n",
    "        fs.rename(generated_file_path, final_output_path)\n",
    "        fs.delete(Path(TEMP_STAGING_DIR), True) \n",
    "        \n",
    "        print(f\"\\n✅ Base integrada salva e renomeada com sucesso para: {final_output_path}\")\n",
    "\n",
    "    else:\n",
    "        print(\"\\nErro: Não foi possível encontrar o arquivo CSV gerado (part-00000-*.csv) no caminho temporário.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nOcorreu um erro durante a renomeação do arquivo no S3: {e}\")\n",
    "\n",
    "# Encerra a sessão Spark\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
