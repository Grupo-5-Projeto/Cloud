{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852080a7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, avg, sum as sum_, first, round, min, when, lit, to_date, to_timestamp, date_format\n",
    "from pyspark.sql.types import DoubleType, IntegerType, StringType, DateType, TimestampType\n",
    "from pyspark.sql.functions import col, year, month, when, avg, round\n",
    "from pyspark.sql.functions import col, year, month, when, avg, round, min\n",
    "\n",
    "\n",
    "# Configurações do Spark\n",
    "conf = SparkConf()\n",
    "conf.set('spark.jars.packages', 'org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.11.901')\n",
    "conf.set('spark.hadoop.fs.s3a.aws.credentials.provider', 'com.amazonaws.auth.InstanceProfileCredentialsProvider')\n",
    "\n",
    "# Criar sessão Spark\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "\n",
    "# Lista de arquivos CSV a serem lidos\n",
    "csv_files = [\n",
    "    'dados_2025_06_02.csv',\n",
    "    'dados_2025_05_27.csv',\n",
    "    'dados_2025_05_28.csv',\n",
    "    'dados_2025_05_29.csv',\n",
    "    'dados_2025_05_30.csv',\n",
    "    'dados_2025_05_31.csv',\n",
    "    'dados_2025_06_01.csv',\n",
    "    'dados_2025_05_20.csv',\n",
    "    'dados_2025_05_21.csv',\n",
    "    'dados_2025_05_22.csv',\n",
    "    'dados_2025_05_23.csv',\n",
    "    'dados_2025_05_24.csv',\n",
    "    'dados_2025_05_25.csv',\n",
    "    'dados_2025_05_26.csv'\n",
    "]\n",
    "\n",
    "# Prefixo do caminho S3\n",
    "s3_prefix = 's3a://bucket-raw-upa-connect-gabriel/'\n",
    "\n",
    "# Criar a lista completa de caminhos S3\n",
    "s3_paths = [s3_prefix + file for file in csv_files]\n",
    "\n",
    "# Ler os CSVs necessários a partir da lista de caminhos\n",
    "TabelaCompleta = spark.read.option('delimiter', ',') \\\n",
    "                             .option('header', 'true') \\\n",
    "                             .option('nullValue', 'null') \\\n",
    "                             .csv(s3_paths) # Agora lê múltiplos arquivos\n",
    "\n",
    "# Ler o CSV, já tratando \"NULL\" como valor nulo\n",
    "upa_df = spark.read.option('delimiter', ',') \\\n",
    "              .option('header', 'true') \\\n",
    "              .csv('s3a://bucket-trusted-upa-connect-gabriel/upa.csv')\n",
    "\n",
    "paciente_df = spark.read.option('delimiter', ',') \\\n",
    "              .option('header', 'true') \\\n",
    "              .csv('s3a://bucket-trusted-upa-connect-gabriel/paciente.csv')\n",
    "\n",
    "sensor_df = spark.read.option('delimiter', ',') \\\n",
    "              .option('header', 'true') \\\n",
    "              .csv('s3a://bucket-trusted-upa-connect-gabriel/sensor.csv')\n",
    "\n",
    "\n",
    "unidadeDeMedida_df = spark.read.option('delimiter', ',') \\\n",
    "              .option('header', 'true') \\\n",
    "              .csv('s3a://bucket-trusted-upa-connect-gabriel/UnidadeDeMedida.csv')\n",
    "\n",
    "temperaturaAmbiente_df = spark.read.option('delimiter', ',') \\\n",
    "              .option('header', 'true') \\\n",
    "              .csv('s3a://bucket-raw-upa-connect-gabriel/clima_tempo.csv')\n",
    "\n",
    "# Converter 'valor' para DoubleType e 'fk_upa' para IntegerType\n",
    "# a = a.withColumn('valor', col('valor').cast(DoubleType())) \\\n",
    "#      .withColumn('fk_paciente', col('fk_paciente').cast(IntegerType()))\n",
    "\n",
    "# TabelaCompleta.show()\n",
    "# upa_df.show()\n",
    "# paciente_df.show(170)\n",
    "# sensor_df.show()\n",
    "# unidadeDeMedida_df.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0c9c71",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, year, month, avg, round, to_date, to_timestamp\n",
    "from pyspark.sql.types import DoubleType, TimestampType\n",
    "\n",
    "# Configurações do Spark\n",
    "conf = SparkConf()\n",
    "conf.set('spark.jars.packages', 'org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.11.901')\n",
    "conf.set('spark.hadoop.fs.s3a.aws.credentials.provider', 'com.amazonaws.auth.InstanceProfileCredentialsProvider')\n",
    "\n",
    "# Criar sessão Spark\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "\n",
    "# Ler CSVs\n",
    "#TabelaCompleta = spark.read.option('delimiter', ',').option('header', 'true').option('nullValue', 'null') \\\n",
    "#    .csv('s3a://bucket-raw-upa-connect-gabriel/tabela_historico_sensor.csv')\n",
    "\n",
    "temperaturaAmbiente_df = spark.read.option('delimiter', ',').option('header', 'true') \\\n",
    "    .csv('s3a://bucket-raw-upa-connect-gabriel/clima_tempo.csv')\n",
    "\n",
    "# =============================\n",
    "# 1. Processar temperatura dos pacientes\n",
    "# =============================\n",
    "df_filtrado_inicial = TabelaCompleta.filter((col(\"fk_sensor\") == 4)) \\\n",
    "    .filter(col('valor').isNotNull())\n",
    "\n",
    "condicao_sensor_temp = (col('fk_sensor') == 4) & (col('valor') >= 34) & (col('valor') <= 42)\n",
    "\n",
    "df_filtrado_final = df_filtrado_inicial.filter(condicao_sensor_temp) \\\n",
    "    .withColumn('valor', col('valor').cast(DoubleType())) \\\n",
    "    .withColumn('data_hora', col('data_hora').cast(TimestampType()))\n",
    "\n",
    "df_resultado_final = df_filtrado_final.withColumn('ano', year(col('data_hora'))) \\\n",
    "    .withColumn('mes', month(col('data_hora'))) \\\n",
    "    .groupBy('ano', 'mes') \\\n",
    "    .agg(round(avg(col('valor')), 1).alias('Temperatura_Media_Paciente')) \\\n",
    "    .orderBy('ano', 'mes')\n",
    "\n",
    "# =============================\n",
    "# 2. Processar temperatura ambiente mensal\n",
    "# =============================\n",
    "temperaturaAmbiente_df = temperaturaAmbiente_df \\\n",
    "    .withColumn('data', to_date(col('data'), 'yyyy-MM-dd')) \\\n",
    "    .withColumn('temperatura_media', col('temperatura_media').cast(DoubleType())) \\\n",
    "    .withColumn('ano', year(col('data'))) \\\n",
    "    .withColumn('mes', month(col('data')))\n",
    "\n",
    "temperaturaAmbiente_df_filtrada = temperaturaAmbiente_df.filter(col('temperatura_media').isNotNull())\n",
    "\n",
    "df_ambiente_mensal = temperaturaAmbiente_df_filtrada.groupBy('ano', 'mes') \\\n",
    "    .agg(round(avg('temperatura_media'), 1).alias('Temperatura_Media_Ambiente'))\n",
    "\n",
    "# =============================\n",
    "# 2.1 Processar quantidade de pessoas na UPA (sensor fk = 1)\n",
    "# =============================\n",
    "df_pessoas_upa = TabelaCompleta.filter((col(\"fk_sensor\") == 1) & col(\"valor\").isNotNull()) \\\n",
    "    .withColumn('valor', col('valor').cast(DoubleType())) \\\n",
    "    .withColumn('data_hora', col('data_hora').cast(TimestampType())) \\\n",
    "    .withColumn('ano', year(col('data_hora'))) \\\n",
    "    .withColumn('mes', month(col('data_hora')))\n",
    "\n",
    "df_pessoas_mensal = df_pessoas_upa.groupBy('ano', 'mes') \\\n",
    "    .agg(round(avg('valor'), 2).alias('Quantidade_Media_Pessoas_UPA'))\n",
    "\n",
    "# =============================\n",
    "# 3. Juntar os três DataFrames e substituir null por 0\n",
    "# =============================\n",
    "df_final_completo = df_resultado_final \\\n",
    "    .join(df_ambiente_mensal, on=['ano', 'mes'], how='left') \\\n",
    "    .join(df_pessoas_mensal, on=['ano', 'mes'], how='left') \\\n",
    "    .fillna(0) \\\n",
    "    .orderBy('ano', 'mes')\n",
    "\n",
    "# =============================\n",
    "# 4. Exibir resultado final\n",
    "# =============================\n",
    "df_final_completo.show(100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5c22a9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df_final_completo.coalesce(1) \\\n",
    "        .write \\\n",
    "        .option('header', 'true') \\\n",
    "        .mode('overwrite') \\\n",
    "        .csv('s3a://bucket-client-upa-connect-gabriel/grafico_temperatura_paciente.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1807b134",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%pip install gspread google-auth-oauthlib boto3 pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d58f87",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip uninstall gspread -y\n",
    "!pip install gspread==5.11.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3582b7b2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8c7e6e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- Instalação de bibliotecas (adicione esta seção no início do seu script) ---\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Função para instalar pacotes\n",
    "def install_package(package):\n",
    "    try:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "        print(f\"'{package}' instalado com sucesso.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao instalar '{package}': {e}\")\n",
    "        # Em ambientes de produção, você pode querer adicionar um exit(1) aqui\n",
    "        # para parar a execução se uma dependência crítica não puder ser instalada.\n",
    "\n",
    "# Instalar as bibliotecas necessárias\n",
    "install_package(\"pandas\")\n",
    "install_package(\"gspread\")\n",
    "install_package(\"google-auth-oauthlib\") # Necessário para google.oauth2.service_account.Credentials\n",
    "install_package(\"boto3\") # Para interagir com o S3\n",
    "# --- Fim da seção de instalação de bibliotecas ---\n",
    "\n",
    "\n",
    "# --- CÓDIGO PARA ENVIAR PARA O GOOGLE SHEETS ---\n",
    "import pandas as pd # <-- MOVIDO PARA AQUI PARA GARANTIR QUE 'pd' ESTEJA NO ESCOPO\n",
    "import gspread\n",
    "from google.oauth2.service_account import Credentials\n",
    "import boto3\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# === CONFIGURAÇÕES PARA O GOOGLE SHEETS ===\n",
    "GOOGLE_SHEET_ID = '1i6BfuZXPOcTp6BFAiVkpktOBym0HtakZacUKfDzu1zI'\n",
    "S3_BUCKET_CREDENTIALS = 'bucket-client-upa-connect-gabriel'\n",
    "S3_KEY_CREDENTIALS = 'credenciais.json'\n",
    "LOCAL_CREDENTIALS_PATH = '/tmp/credenciais.json'\n",
    "\n",
    "# === AUTENTICAÇÃO GOOGLE ===\n",
    "print(f\"Baixando credenciais do S3: s3://{S3_BUCKET_CREDENTIALS}/{S3_KEY_CREDENTIALS} para {LOCAL_CREDENTIALS_PATH}...\")\n",
    "s3 = boto3.client('s3')\n",
    "try:\n",
    "    s3.download_file(S3_BUCKET_CREDENTIALS, S3_KEY_CREDENTIALS, LOCAL_CREDENTIALS_PATH)\n",
    "    print(\"Credenciais baixadas com sucesso.\")\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao baixar credenciais do S3: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "scopes = [\"https://www.googleapis.com/auth/spreadsheets\", \"https://www.googleapis.com/auth/drive\"]\n",
    "credenciais = Credentials.from_service_account_file(LOCAL_CREDENTIALS_PATH, scopes=scopes)\n",
    "cliente = gspread.authorize(credenciais)\n",
    "planilha = cliente.open_by_key(GOOGLE_SHEET_ID)\n",
    "\n",
    "# === ENVIA O DATAFRAME FINAL PARA A NOVA ABA \"umidade\" ===\n",
    "NOME_ABA_TESTE = \"temperatura_paciente\"\n",
    "print(f\"Enviando DataFrame final para a aba '{NOME_ABA_TESTE}'...\")\n",
    "\n",
    "# Converter Spark DataFrame para Pandas DataFrame\n",
    "try:\n",
    "    df_pandas = df_final_completo.toPandas()\n",
    "    print(\"DataFrame Spark convertido para Pandas com sucesso.\")\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao converter Spark DataFrame para Pandas: {e}\")\n",
    "    print(\"O DataFrame pode ser muito grande para a memória do driver ou a sessão Spark está inválida.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# --- NOVO TRECHO: Ajustar tipagem das colunas para o Google Sheets ---\n",
    "# Apenas formata colunas de data/hora para string ISO 8601.\n",
    "# As demais colunas (numéricas, strings, etc.) são mantidas em seus tipos nativos no Pandas.\n",
    "# O gspread e o Google Sheets farão a inferência de tipo para elas.\n",
    "\n",
    "# # Coluna 'data_chegada': assegurar que é string DD/MM/YYYY\n",
    "if 'data' in df_pandas.columns:\n",
    "    if pd.api.types.is_datetime64_any_dtype(df_pandas['data']):\n",
    "        df_pandas['data'] = df_pandas['data'].dt.strftime('%d/%m/%Y') # <-- MUDANÇA AQUI\n",
    "    elif pd.api.types.is_string_dtype(df_pandas['data']):\n",
    "        # Tentar converter strings que podem não estar em formato padrão para datetime,\n",
    "        # e depois formatar para DD/MM/YYYY. 'errors=coerce' transforma não-datas em NaT.\n",
    "        df_pandas['data'] = pd.to_datetime(df_pandas['data'], errors='coerce').dt.strftime('%d/%m/%Y') # <-- MUDANÇA AQUI\n",
    "    # Preencher valores nulos (NaT) com string vazia, pois None também pode causar problemas.\n",
    "    df_pandas['data'] = df_pandas['data'].fillna('')\n",
    "\n",
    "# # Coluna 'horario_chegada': assegurar que é string HH:MM:SS\n",
    "if 'hora' in df_pandas.columns:\n",
    "    if pd.api.types.is_datetime64_any_dtype(df_pandas['hora']):\n",
    "        df_pandas['hora'] = df_pandas['hora'].dt.strftime('%H:%M:%S')\n",
    "    elif pd.api.types.is_string_dtype(df_pandas['hora']):\n",
    "        # Se já é string (como no seu caso vindo do Spark), apenas garanta que nulos sejam preenchidos.\n",
    "        df_pandas['hora'] = df_pandas['hora'].fillna('')\n",
    "\n",
    "\n",
    "# Nenhuma conversão global df_pandas.astype(str) é necessária aqui.\n",
    "# gspread é capaz de lidar com int, float, bool, e strings diretamente.\n",
    "\n",
    "dados = [df_pandas.columns.tolist()] + df_pandas.values.tolist()\n",
    "\n",
    "\n",
    "try:\n",
    "    aba = planilha.worksheet(NOME_ABA_TESTE)\n",
    "    aba.clear()\n",
    "    print(f\"Aba '{NOME_ABA_TESTE}' encontrada e limpa.\")\n",
    "except gspread.exceptions.WorksheetNotFound:\n",
    "    aba = planilha.add_worksheet(title=NOME_ABA_TESTE, rows=str(len(dados) + 100), cols=str(len(df_pandas.columns) + 10))\n",
    "    print(f\"Aba '{NOME_ABA_TESTE}' criada.\")\n",
    "\n",
    "try:\n",
    "    aba.update('A1', dados)\n",
    "    print(f\"Dados do DataFrame final enviados com sucesso para a aba '{NOME_ABA_TESTE}'.\")\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao atualizar a aba do Google Sheets: {e}\")\n",
    "\n",
    "# Opcional: Remover o arquivo de credenciais temporário\n",
    "if os.path.exists(LOCAL_CREDENTIALS_PATH):\n",
    "    os.remove(LOCAL_CREDENTIALS_PATH)\n",
    "    print(f\"Arquivo de credenciais temporário '{LOCAL_CREDENTIALS_PATH}' removido.\")\n",
    "\n",
    "# Parar a sessão Spark\n",
    "# spark.stop()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
