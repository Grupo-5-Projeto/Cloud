{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7101fb24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.7/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      "com.amazonaws#aws-java-sdk-bundle added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-c15fbd62-15c4-40ff-a413-0417472975ee;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.4 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.12.262 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.3.4/hadoop-aws-3.3.4.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-aws;3.3.4!hadoop-aws.jar (55ms)\n",
      "downloading https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.12.262/aws-java-sdk-bundle-1.12.262.jar ...\n",
      "\t[SUCCESSFUL ] com.amazonaws#aws-java-sdk-bundle;1.12.262!aws-java-sdk-bundle.jar (3265ms)\n",
      "downloading https://repo1.maven.org/maven2/org/wildfly/openssl/wildfly-openssl/1.0.7.Final/wildfly-openssl-1.0.7.Final.jar ...\n",
      "\t[SUCCESSFUL ] org.wildfly.openssl#wildfly-openssl;1.0.7.Final!wildfly-openssl.jar (24ms)\n",
      ":: resolution report :: resolve 2076ms :: artifacts dl 3360ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.12.262 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.4 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\t:: evicted modules:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.901 by [com.amazonaws#aws-java-sdk-bundle;1.12.262] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   4   |   3   |   3   |   1   ||   3   |   3   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-c15fbd62-15c4-40ff-a413-0417472975ee\n",
      "\tconfs: [default]\n",
      "\t3 artifacts copied, 0 already retrieved (275421kB/670ms)\n",
      "25/04/28 20:27:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "/usr/local/lib/python3.7/site-packages/pyspark/context.py:317: FutureWarning: Python 3.7 support is deprecated in Spark 3.4.\n",
      "  warnings.warn(\"Python 3.7 support is deprecated in Spark 3.4.\", FutureWarning)\n",
      "25/04/28 20:27:11 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+-------------------+-----+-----------+\n",
      "|id_temperatura_paciente|          data_hora|valor|fk_paciente|\n",
      "+-----------------------+-------------------+-----+-----------+\n",
      "|                      1|2025-04-26 21:31:43| 36.9|          7|\n",
      "|                      2|2025-04-26 21:31:42| 36.9|          7|\n",
      "|                      3|2025-04-26 21:31:41| 36.9|          7|\n",
      "|                      4|2025-04-26 21:36:43| 36.5|         35|\n",
      "|                      5|2025-04-26 21:36:42| 36.5|         35|\n",
      "|                      6|2025-04-26 21:36:41| 36.6|         35|\n",
      "|                      7|2025-04-26 21:41:43| 36.8|         11|\n",
      "|                      8|2025-04-26 21:41:42| 36.7|         11|\n",
      "|                      9|2025-04-26 21:41:41| 36.9|         11|\n",
      "|                     10|2025-04-26 21:46:43| 36.4|         92|\n",
      "|                     11|2025-04-26 21:46:42| 36.3|         92|\n",
      "|                     12|2025-04-26 21:46:41| 36.3|         92|\n",
      "|                     13|2025-04-26 21:51:43| null|         69|\n",
      "|                     14|2025-04-26 21:51:42| 37.2|         69|\n",
      "|                     15|2025-04-26 21:51:41| 37.2|         69|\n",
      "|                     16|2025-04-26 21:56:43| 37.4|         57|\n",
      "|                     17|2025-04-26 21:56:42| 37.3|         57|\n",
      "|                     18|2025-04-26 21:56:41| 37.4|         57|\n",
      "|                     19|2025-04-26 22:01:43| 54.3|          6|\n",
      "|                     20|2025-04-26 22:01:42| 54.5|          6|\n",
      "+-----------------------+-------------------+-----+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, avg, sum as sum_, first, round\n",
    "from pyspark.sql.types import DoubleType, IntegerType\n",
    "\n",
    "# Configurações do Spark\n",
    "conf = SparkConf()\n",
    "conf.set('spark.jars.packages', 'org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.11.901')\n",
    "conf.set('spark.hadoop.fs.s3a.aws.credentials.provider', 'com.amazonaws.auth.InstanceProfileCredentialsProvider')\n",
    "\n",
    "# Criar sessão Spark\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "\n",
    "# Ler o CSV, já tratando \"NULL\" como valor nulo\n",
    "a = spark.read.option('delimiter', ',') \\\n",
    "              .option('header', 'true') \\\n",
    "              .option('nullValue', 'NULL') \\\n",
    "              .csv('s3a://bucket-raw-upa-connect/temperatura_paciente.csv')\n",
    "\n",
    "# Converter 'valor' para DoubleType e 'fk_paciente' para IntegerType\n",
    "a = a.withColumn('valor', col('valor').cast(DoubleType())) \\\n",
    "     .withColumn('fk_paciente', col('fk_paciente').cast(IntegerType()))\n",
    "\n",
    "a.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "543d43bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|qtd_nulls|\n",
      "+---------+\n",
      "|       82|\n",
      "+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "nulls_valor = a.select(sum_(col(\"valor\").isNull().cast(\"decimal\")).alias(\"qtd_nulls\"))\n",
    "\n",
    "# Mostra o resultado\n",
    "nulls_valor.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "069d2dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+-------------------+-----+-----------+\n",
      "|id_temperatura_paciente|          data_hora|valor|fk_paciente|\n",
      "+-----------------------+-------------------+-----+-----------+\n",
      "|                     13|2025-04-26 21:51:43| null|         69|\n",
      "|                     22|2025-04-26 22:06:43| null|         65|\n",
      "|                     43|2025-04-26 22:41:43| null|         72|\n",
      "|                     46|2025-04-26 22:46:43| null|         26|\n",
      "|                     55|2025-04-26 23:01:43| null|         83|\n",
      "|                     61|2025-04-26 23:11:43| null|         72|\n",
      "|                     73|2025-04-26 23:31:43| null|         73|\n",
      "|                     88|2025-04-26 23:56:43| null|         53|\n",
      "|                     97|2025-04-27 00:11:43| null|         41|\n",
      "|                    100|2025-04-27 00:16:43| null|         59|\n",
      "|                    109|2025-04-27 00:31:43| null|         60|\n",
      "|                    115|2025-04-27 00:41:43| null|        113|\n",
      "|                    145|2025-04-27 01:31:43| null|         42|\n",
      "|                    157|2025-04-27 01:51:43| null|         58|\n",
      "|                    160|2025-04-27 01:56:43| null|         82|\n",
      "|                    163|2025-04-27 02:01:43| null|         80|\n",
      "|                    169|2025-04-27 02:11:43| null|         14|\n",
      "|                    190|2025-04-27 02:46:43| null|        122|\n",
      "|                    223|2025-04-27 03:41:43| null|         69|\n",
      "|                    232|2025-04-27 03:56:43| null|         60|\n",
      "+-----------------------+-------------------+-----+-----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-----------------------+-------------------+-----+-----------+\n",
      "|id_temperatura_paciente|          data_hora|valor|fk_paciente|\n",
      "+-----------------------+-------------------+-----+-----------+\n",
      "|                      1|2025-04-26 21:31:43| 36.9|          7|\n",
      "|                      2|2025-04-26 21:31:42| 36.9|          7|\n",
      "|                      3|2025-04-26 21:31:41| 36.9|          7|\n",
      "|                      4|2025-04-26 21:36:43| 36.5|         35|\n",
      "|                      5|2025-04-26 21:36:42| 36.5|         35|\n",
      "|                      6|2025-04-26 21:36:41| 36.6|         35|\n",
      "|                      7|2025-04-26 21:41:43| 36.8|         11|\n",
      "|                      8|2025-04-26 21:41:42| 36.7|         11|\n",
      "|                      9|2025-04-26 21:41:41| 36.9|         11|\n",
      "|                     10|2025-04-26 21:46:43| 36.4|         92|\n",
      "|                     11|2025-04-26 21:46:42| 36.3|         92|\n",
      "|                     12|2025-04-26 21:46:41| 36.3|         92|\n",
      "|                     14|2025-04-26 21:51:42| 37.2|         69|\n",
      "|                     15|2025-04-26 21:51:41| 37.2|         69|\n",
      "|                     16|2025-04-26 21:56:43| 37.4|         57|\n",
      "|                     17|2025-04-26 21:56:42| 37.3|         57|\n",
      "|                     18|2025-04-26 21:56:41| 37.4|         57|\n",
      "|                     19|2025-04-26 22:01:43| 54.3|          6|\n",
      "|                     20|2025-04-26 22:01:42| 54.5|          6|\n",
      "|                     21|2025-04-26 22:01:41| 54.4|          6|\n",
      "+-----------------------+-------------------+-----+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Nulls\n",
    "nulls = a.filter(col('valor').isNull())\n",
    "\n",
    "nulls.show()\n",
    "\n",
    "# Remover linhas onde a coluna 'valor' está nula\n",
    "a = a.filter(col('valor').isNotNull())\n",
    "\n",
    "a.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d0033c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outliers encontrados:\n",
      "+-----------------------+-------------------+-----+-----------+\n",
      "|id_temperatura_paciente|          data_hora|valor|fk_paciente|\n",
      "+-----------------------+-------------------+-----+-----------+\n",
      "|                     19|2025-04-26 22:01:43| 54.3|          6|\n",
      "|                     20|2025-04-26 22:01:42| 54.5|          6|\n",
      "|                     21|2025-04-26 22:01:41| 54.4|          6|\n",
      "|                    187|2025-04-27 02:41:43| 14.1|         70|\n",
      "|                    188|2025-04-27 02:41:42| 14.1|         70|\n",
      "|                    189|2025-04-27 02:41:41| 13.9|         70|\n",
      "|                    427|2025-04-27 09:21:43| 80.8|         43|\n",
      "|                    428|2025-04-27 09:21:42| 80.5|         43|\n",
      "|                    429|2025-04-27 09:21:41| 81.2|         43|\n",
      "|                    613|2025-04-27 14:31:43| 89.9|         52|\n",
      "|                    614|2025-04-27 14:31:42| 90.2|         52|\n",
      "|                    615|2025-04-27 14:31:41| 89.7|         52|\n",
      "|                    751|2025-04-27 18:21:43| -5.6|         91|\n",
      "|                    752|2025-04-27 18:21:42| -5.7|         91|\n",
      "|                    753|2025-04-27 18:21:41| -5.7|         91|\n",
      "|                    796|2025-04-27 19:36:43| 86.0|        110|\n",
      "|                    797|2025-04-27 19:36:42| 86.1|        110|\n",
      "|                    798|2025-04-27 19:36:41| 85.6|        110|\n",
      "+-----------------------+-------------------+-----+-----------+\n",
      "\n",
      "DataFrame final:\n",
      "+-----------------------+-------------------+-----+-----------+\n",
      "|id_temperatura_paciente|          data_hora|valor|fk_paciente|\n",
      "+-----------------------+-------------------+-----+-----------+\n",
      "|                      1|2025-04-26 21:31:43| 36.9|          7|\n",
      "|                      2|2025-04-26 21:31:42| 36.9|          7|\n",
      "|                      3|2025-04-26 21:31:41| 36.9|          7|\n",
      "|                      4|2025-04-26 21:36:43| 36.5|         35|\n",
      "|                      5|2025-04-26 21:36:42| 36.5|         35|\n",
      "|                      6|2025-04-26 21:36:41| 36.6|         35|\n",
      "|                      7|2025-04-26 21:41:43| 36.8|         11|\n",
      "|                      8|2025-04-26 21:41:42| 36.7|         11|\n",
      "|                      9|2025-04-26 21:41:41| 36.9|         11|\n",
      "|                     10|2025-04-26 21:46:43| 36.4|         92|\n",
      "|                     11|2025-04-26 21:46:42| 36.3|         92|\n",
      "|                     12|2025-04-26 21:46:41| 36.3|         92|\n",
      "|                     14|2025-04-26 21:51:42| 37.2|         69|\n",
      "|                     15|2025-04-26 21:51:41| 37.2|         69|\n",
      "|                     16|2025-04-26 21:56:43| 37.4|         57|\n",
      "|                     17|2025-04-26 21:56:42| 37.3|         57|\n",
      "|                     18|2025-04-26 21:56:41| 37.4|         57|\n",
      "|                     23|2025-04-26 22:06:42| 37.1|         65|\n",
      "|                     24|2025-04-26 22:06:41| 36.3|         65|\n",
      "|                     25|2025-04-26 22:11:43| 36.4|         28|\n",
      "+-----------------------+-------------------+-----+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Mostrar possíveis outliers\n",
    "outliers = a.filter((col('valor') <= 34) | (col('valor') > 42))\n",
    "print('Outliers encontrados:')\n",
    "outliers.show()\n",
    "\n",
    "# Remover os outliers\n",
    "a = a.filter((col('valor') >= 34) & (col('valor') <= 42))\n",
    "\n",
    "# Mostrar resultado final (sem nulos e sem outliers)\n",
    "print('DataFrame final:')\n",
    "a.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d968cd25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame final:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------+-----------------+\n",
      "|fk_paciente|          data_hora|media_temperatura|\n",
      "+-----------+-------------------+-----------------+\n",
      "|          7|2025-04-26 21:31:43|             36.9|\n",
      "|         35|2025-04-26 21:36:43|             36.8|\n",
      "|         11|2025-04-26 21:41:43|             36.8|\n",
      "|         92|2025-04-26 21:46:43|             36.7|\n",
      "|         69|2025-04-26 21:51:42|             37.1|\n",
      "|         57|2025-04-26 21:56:43|             37.0|\n",
      "|         65|2025-04-26 22:06:42|             37.1|\n",
      "|         28|2025-04-26 22:11:43|             36.6|\n",
      "|         44|2025-04-26 22:16:43|             36.9|\n",
      "|        104|2025-04-26 22:21:43|             36.1|\n",
      "|         16|2025-04-26 22:26:43|             37.0|\n",
      "|        127|2025-04-26 22:31:43|             37.0|\n",
      "|         72|2025-04-26 22:41:42|             36.6|\n",
      "|         26|2025-04-26 22:46:42|             37.0|\n",
      "|          5|2025-04-26 22:51:43|             36.7|\n",
      "|        111|2025-04-26 22:56:43|             36.8|\n",
      "|         83|2025-04-26 23:01:42|             36.8|\n",
      "|         70|2025-04-26 23:06:43|             36.6|\n",
      "|         60|2025-04-26 23:16:43|             37.0|\n",
      "|         90|2025-04-26 23:26:43|             36.9|\n",
      "+-----------+-------------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Agrupa por paciente, pega a primeira data_hora e calcula a média do valor\n",
    "a_agrupada = a.groupBy('fk_paciente').agg(\n",
    "    first('data_hora').alias('data_hora'),\n",
    "    round(avg('valor'), 1).alias('media_temperatura')\n",
    ")\n",
    "\n",
    "# Ordena pelo fk_paciente (se quiser)\n",
    "a_agrupada = a_agrupada.orderBy('data_hora')\n",
    "\n",
    "# Exibe o resultado\n",
    "print('DataFrame final:')\n",
    "a_agrupada.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "941db253",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o107.csv.\n: java.util.NoSuchElementException: None.get\n\tat scala.None$.get(Option.scala:529)\n\tat scala.None$.get(Option.scala:527)\n\tat org.apache.spark.sql.execution.datasources.BasicWriteJobStatsTracker$.metrics(BasicWriteStatsTracker.scala:239)\n\tat org.apache.spark.sql.execution.command.DataWritingCommand.metrics(DataWritingCommand.scala:55)\n\tat org.apache.spark.sql.execution.command.DataWritingCommand.metrics$(DataWritingCommand.scala:55)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.metrics$lzycompute(InsertIntoHadoopFsRelationCommand.scala:47)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.metrics(InsertIntoHadoopFsRelationCommand.scala:47)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.metrics$lzycompute(commands.scala:109)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.metrics(commands.scala:109)\n\tat org.apache.spark.sql.execution.SparkPlanInfo$.fromSparkPlan(SparkPlanInfo.scala:63)\n\tat org.apache.spark.sql.execution.SparkPlanInfo$.$anonfun$fromSparkPlan$3(SparkPlanInfo.scala:75)\n\tat scala.collection.immutable.List.map(List.scala:293)\n\tat org.apache.spark.sql.execution.SparkPlanInfo$.fromSparkPlan(SparkPlanInfo.scala:75)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:115)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\n\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:847)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_15473/3427389421.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'header'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'true'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'overwrite'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m's3a://bucket-trusted-upa-connect/temperatura_paciente_tratado.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001b[0m\n\u001b[1;32m   1797\u001b[0m             \u001b[0mlineSep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlineSep\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1798\u001b[0m         )\n\u001b[0;32m-> 1799\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1800\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1801\u001b[0m     def orc(\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1323\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o107.csv.\n: java.util.NoSuchElementException: None.get\n\tat scala.None$.get(Option.scala:529)\n\tat scala.None$.get(Option.scala:527)\n\tat org.apache.spark.sql.execution.datasources.BasicWriteJobStatsTracker$.metrics(BasicWriteStatsTracker.scala:239)\n\tat org.apache.spark.sql.execution.command.DataWritingCommand.metrics(DataWritingCommand.scala:55)\n\tat org.apache.spark.sql.execution.command.DataWritingCommand.metrics$(DataWritingCommand.scala:55)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.metrics$lzycompute(InsertIntoHadoopFsRelationCommand.scala:47)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.metrics(InsertIntoHadoopFsRelationCommand.scala:47)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.metrics$lzycompute(commands.scala:109)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.metrics(commands.scala:109)\n\tat org.apache.spark.sql.execution.SparkPlanInfo$.fromSparkPlan(SparkPlanInfo.scala:63)\n\tat org.apache.spark.sql.execution.SparkPlanInfo$.$anonfun$fromSparkPlan$3(SparkPlanInfo.scala:75)\n\tat scala.collection.immutable.List.map(List.scala:293)\n\tat org.apache.spark.sql.execution.SparkPlanInfo$.fromSparkPlan(SparkPlanInfo.scala:75)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:115)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\n\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:847)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/py4j/clientserver.py\", line 540, in send_command\n",
      "    \"Error while sending or receiving\", e, proto.ERROR_ON_RECEIVE)\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    }
   ],
   "source": [
    "a_agrupada.coalesce(1) \\\n",
    "    .write \\\n",
    "    .option('header', 'true') \\\n",
    "    .mode('overwrite') \\\n",
    "    .csv('s3a://bucket-trusted-upa-connect/temperatura_paciente_tratado.csv')\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c3fa96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
